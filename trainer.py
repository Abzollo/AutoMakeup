
import torch
import torch.nn as nn
import torchvision.utils as vutils
import matplotlib.pyplot as plt
import matplotlib.animation as animation

REAL = 1
FAKE = 0


class MakeupNetTrainer:
	"""The trainer for MakeupNet."""

	def __init__(self, model, dataset, num_gpu=0, num_epochs=5,
		batch_size=4, optimizer_name="sgd", lr=1e-4, momentum=0.9):
		"""
		Initializes MakeupNetTrainer.

		Args:
			model: The makeup net.
			dataset: The makeup dataset.
			num_gpu: Number of GPUs to use for training.
			num_epochs: Number of epochs to train.
			batch_size: Size of the batch. Must be > num_gpu.
			optimizer_name: The name of the optimizer to use (e.g. "sgd").
			lr: The learning rate.
			momentum: The momentum of used in the optimizer, if applicable.
		"""

		# Initialize training parameters
		self.num_gpu = num_gpu
		self.num_epochs = num_epochs
		self.batch_size = batch_size
		self.optimizer_name = optimizer_name
		self.lr = lr
		self.momentum = momentum

		# Initialize device (cpu or gpu)
		if torch.cuda.is_available() and self.num_gpu > 0:
			self.device = torch.device("cuda:0")
		else:
			self.device = torch.device("cpu")

		# Initialize model on device
		self.model = model.to(self.device)
		# Parallelize model if we're using multiple gpus
		if self.device.type == "cuda" and self.num_gpu > 1:
			self.model = nn.DataParallel(self.model, list(range(self.num_gpu)))

		# Initialize optimizers for generator and discriminator
		self.D_optim = torch.optim.Adam(self.model.D.parameters(), lr=self.lr)
		self.G_optim = torch.optim.Adam(self.model.G.parameters(), lr=self.lr)

		# Define criterion
		self.criterion = nn.BCELoss()

		# Initialize data loader
		self.data_loader = torch.utils.data.DataLoader(dataset,
			batch_size=self.batch_size, shuffle=True, num_workers=2)
		self.len_dataset = len(dataset)

		# Initialize variables used for tracking loss and progress
		self.D_losses = []
		self.G_losses = []
		self.fixed_sample_progress = []
		self.fixed_noise = torch.randn(64, self.model.num_latent, 1, 1, device=self.device)


	def train(self):
		"""
		Trains `self.model` on `self.dataset`.
		"""

		print("Starting Training Loop...")
		# For each epoch
		for epoch in range(self.num_epochs):
			for batch_index, sample in enumerate(self.data_loader):

				# Sample real images
				real = sample["before"].to(self.device)

				# Calculate latent vector size
				batch_size = real.size()[0]
				latent_size = torch.Size([batch_size, self.model.num_latent, 1, 1])

				# Sample fake images from a random latent vector
				latent = torch.randn(latent_size, device=self.device)
				fake = self.model.G(latent)

				# Perform training step on discriminator and generator
				D_of_x, D_of_G_z1 = self.D_step(real, fake)
				D_of_G_z2 = self.G_step(fake)

				# Calculate index of data point
				index = batch_index * batch_size

				# Report training stats @TODO replace 50 with var (same below)
				if batch_index % 50 == 0:
					self.report_training_stats(index, epoch, D_of_x, D_of_G_z1, D_of_G_z2)

				# Check generator's progress by recording its output on a fixed input
				if batch_index % 50 == 0:
					self.check_progress_of_generator(self.model.G)


		print("Finished training.")
		self.check_progress_of_generator(self.model.G)  # check progress at the end

		# Plot losses of D and G
		self.plot_losses()

		# Create an animation of the generator's progress
		self.create_progress_animation(self.fixed_sample_progress, "test.mp4")


	def D_step(self, real, fake):
		"""
		Trains the discriminator.

		Args:
			real: The real image sampled from the dataset.
			fake: The fake image generated by the generator.

		Returns:
			A tuple containing the mean classification of the discriminator on
			the real images and the fake images, respectively.
		"""

		# Zero gradients
		self.D_optim.zero_grad()

		# Initialize real and fake labels
		batch_size = real.size()[0]
		real_label = torch.full([batch_size], REAL, device=self.device)
		fake_label = torch.full([batch_size], FAKE, device=self.device)

		# Classify real images and calculate error
		D_on_real = self.model.D(real).view(-1)
		D_error_on_real = self.criterion(D_on_real, real_label)

		# Classify fake images and calculate error (don't pass gradients to G)
		D_on_fake = self.model.D(fake.detach()).view(-1)
		D_error_on_fake = self.criterion(D_on_fake, fake_label)

		# Calculate gradients from the error on real and fake images
		D_error = D_error_on_real + D_error_on_fake
		D_error.backward()
		
		# Update
		self.D_optim.step()

		# Record loss
		self.D_losses.append(D_error.item())

		return (
			D_on_real.mean().item(),
			D_on_fake.mean().item(),
		)


	def G_step(self, fake):
		"""
		Trains the generator.

		Args:
			fake: The fake image generated by the generator.

		Returns:
			The mean classification of the discriminator on the fake images.
		"""

		# Zero gradients
		self.G_optim.zero_grad()

		# Note that we use real labels because we want the generator to
		# output more realistic images
		batch_size = fake.size()[0]
		real_label = torch.full([batch_size], REAL, device=self.device)

		# Classify fake images and calculate generator's error
		# (i.e. how far it is from real)
		D_on_fake = self.model.D(fake).view(-1)
		G_error = self.criterion(D_on_fake, real_label)

		# Calculate gradients and update
		G_error.backward()
		self.G_optim.step()

		# Record loss
		self.G_losses.append(G_error.item())

		return D_on_fake.mean().item()



	def check_progress_of_generator(self, generator):
		"""
		Check generator's output on a fixed latent vector and record it.

		Args:
			generator: The generator which we want to check.
		"""

		with torch.no_grad():
			fixed_fake = generator(self.fixed_noise).detach().cpu()

		self.fixed_sample_progress.append(
			vutils.make_grid(fixed_fake, padding=2, normalize=True)
		)


	def report_training_stats(self, index, epoch, D_of_x, D_of_G_z1, D_of_G_z2, precision=4):
		"""
		Reports/prints the training stats to the console.

		Args:
			index: Index of the current data point.
			epoch: Current epoch.
			D_of_x: Mean classification of the discriminator on the real images.
			D_of_G_z1: Mean classification of the discriminator on the fake images (D_step).
			D_of_G_z2: Mean classification of the discriminator on the fake images (G_step).
			precision: Precision of the float numbers reported.
		"""

		report = \
			"[{epoch}/{num_epochs}][{index}/{len_dataset}]\t" \
			"Loss of D = {D_loss:.{p}f}\t" \
			"Loss of G = {G_loss:.{p}f}\t" \
			"D(x) = {D_of_x:.{p}f}\t" \
			"D(G(z)) = {D_of_G_z1:.{p}f} / {D_of_G_z2:.{p}f}"

		stats = {
			"epoch": epoch,
			"num_epochs": self.num_epochs,
			"index": index,
			"len_dataset": self.len_dataset,
			"D_loss": self.D_losses[-1],
			"G_loss": self.G_losses[-1],
			"D_of_x": D_of_x,
			"D_of_G_z1": D_of_G_z1,
			"D_of_G_z2": D_of_G_z2,
			"p": precision,
		}

		print(report.format(**stats))


	def plot_losses(self):
		"""
		Plots the losses of the discriminator and the generator.
		"""
		plt.figure(figsize=(10,5))
		plt.title("Generator and Discriminator Loss During Training")
		plt.plot(self.D_losses, label="D")
		plt.plot(self.G_losses, label="G")
		plt.xlabel("iterations")
		plt.ylabel("loss")
		plt.legend()
		plt.show()


	def create_progress_animation(self, progress_images, file_name):
		"""
		Creates a video of the progress of the generator on a fixed latent vector.
		"""
		fig = plt.figure(figsize=(8,8))
		plt.axis("off")
		ims = [[plt.imshow(img.permute(1,2,0), animated=True)] for img in progress_images]
		ani = animation.ArtistAnimation(fig, ims, blit=True)
		ani.save(file_name)


